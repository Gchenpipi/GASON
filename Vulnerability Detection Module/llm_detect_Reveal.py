import time
import pandas as pd
from tqdm import tqdm
import os
import json
import csv
import logging
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from sklearn.metrics import matthews_corrcoef, roc_auc_score

# === Ollama API ===
OLLAMA_API_URL = "http://127.0.0.1:11434/api/chat"

templates = {
    1: 'You are now an excellent programmer.'
       ' You are conducting a function vulnerability detection task for C/C++ language.',
    2: ' In the above code snippet, check for potential security vulnerabilities and output either "Vulnerable" or "Non-vulnerable".',
    3: ' The following are code slices extracted from the source code above. '
       'The provided snippets may not be complete, but they contain all relevant information.'
       ' Additional code slices are provided to assist your analysis.'
       ' These snippets highlight potentially relevant parts of the program.'
       ' Use them only as supporting information.',
    4: ' Here is an example for you to learn from,',
    5: ' You only need to output the initially given code is Vulnerable or Non-vulnerable, without explanation.',
}



def call_ollama_api(model, prompt, num_ctx=8192):
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "options": {"num_ctx": num_ctx}
    }
    response = requests.post(OLLAMA_API_URL, json=payload)
    response.raise_for_status()
    return response.json()



def build_enhanced_section(row, max_slices=20, max_chars_per_slice=1000):
    slice_parts = []
    for i in range(max_slices):
        key = f"enhanced_code_{i}"
        snippet = row.get(key)
        if not snippet or str(snippet).strip() in ["", "nan", "None"]:
            continue
        s = str(snippet).strip()
        if len(s) > max_chars_per_slice:
            s = s[:max_chars_per_slice] + "\n/* ...truncated... */"
        slice_parts.append(f"Enhanced snippet {i}:\n{s}\n")

    if not slice_parts:
        return ""
    return (
        "The following enhanced code slices were extracted for this file:\n\n"
        + "\n---\n".join(slice_parts)
        + "\n"
    )



def load_restructured_results(restructured_path, max_slices=50):
    with open(restructured_path, 'r') as f:
        data = json.load(f)
    code_to_slices = {}
    for item in data:
        filename = item.get("file_name")
        if not filename:
            continue
        code_to_slices[filename] = {
            f"enhanced_code_{i}": item.get(f"enhanced_code_{i}", "")
            for i in range(max_slices)
        }
    return code_to_slices



def process_sample(row, restructured_data, max_sim_slices=10):
    inputCode = row.get('Code', '')[:4000]
    inputex = row.get('sim_code', '')[:4000]
    sim_label = row.get('sim_label', '')


    trans_label = 'Unknown'
    if sim_label == '1':
        trans_label = 'Vulnerable'
    elif sim_label == '0':
        trans_label = 'Non-vulnerable'


    enhanced_section_main = build_enhanced_section(row)


    sim_filename = row.get('sim_Filename', None)
    enhanced_section_sim = ""
    if sim_filename and sim_filename in restructured_data:
        enhanced_section_sim = build_enhanced_section(
            restructured_data[sim_filename],
            max_slices=max_sim_slices
        )


    prompt_content = f"""
    You are an expert in real-world C/C++ vulnerability detection on large production systems 
    (such as Debian and Chromium, the source of the REVEAL dataset).
    Your task is to classify each sample as Vulnerable or Non-vulnerable with **balanced recall and precision**.
    You MUST NOT default to Non-vulnerable.

    Your inputs:
    1. MAIN FUNCTION — full C/C++ function
    2. ENHANCED SLICES — CPG-derived slices (may be noisy; use softly, not strictly)
    3. SIMILAR EXAMPLE — structurally similar reference (never copy its label)

    ============================================================
    CORE DECISION PRINCIPLE (High Recall + Stable Precision)
    ============================================================
    Classify as **Vulnerable** when ANY of the following are true:

    1. The function performs a memory/pointer/index/buffer operation AND  
       the controlling value comes from external/dynamic input OR depends on runtime state,  
       **unless clear in-code constraints prevent unsafe ranges**.

    2. A pointer, index, buffer length, or allocation size may depend on external/dynamic input AND  
       its safety (bounds, NULL-checks, lifetime, aliasing) is  
       **uncertain, ambiguous, or not enforceable from this code alone**.

    3. Slices show a plausible data flow from unvalidated input **to the same variable**  
       used in memory/pointer/index/buffer operations, **and this flow is not contradicted by the main code**.

    4. There exists a reasonable execution path where a memory-related operation  
       could become unsafe due to missing or incomplete validation.

    ===================================================================
    NON-VULNERABLE CRITERIA (Requires ALL)
    ===================================================================
    Classify as **Non-vulnerable** only when:

    - Memory/pointer/index/buffer operations have explicit and relevant checks, AND
    - No visible path allows untrusted/dynamic input to influence memory behavior, AND
    - Slices do not show a consistent or code-aligned risky flow to a memory-relevant variable, AND
    - Any remaining uncertainty does NOT affect memory/pointer/index/buffer safety.

    ============================================================
    FALSE POSITIVE AVOIDANCE (Precision Protection)
    ============================================================
    Do NOT mark as Vulnerable solely because of:
    - structural or branching complexity,
    - harmless pointer usage without unsafe access,
    - slices involving unrelated variables or contradicting main code behavior,
    - external input that never participates in memory/index operations,
    - loops with safe or clearly bounded iteration,
    - pure printing/logging/text formatting,
    - arithmetic unrelated to memory safety.

    ============================================================
    SLICE INTERPRETATION (Soft Filtering, Recall-Preserving)
    ============================================================
    Use slices only when:
    - they reference the **same variable** involved in memory-sensitive operations, AND
    - they provide a plausible and code-consistent flow from unvalidated or dynamic input.

    Otherwise treat slices as **weak/noisy signals**, not evidence.

    ============================================================
    UNCERTAINTY POLICY (Recall Preservation)
    ============================================================
    If uncertainty affects memory, pointer, index, or buffer safety → **Vulnerable**.  
    If uncertainty affects only non-memory/semantic logic → **Non-vulnerable**.

    ============================================================
    OUTPUT FORMAT (STRICT)
    Return EXACTLY one word:
    Vulnerable
    or
    Non-vulnerable

    ======== MAIN FUNCTION ========
    {inputCode}

    ======== ENHANCED SLICES (from CPG) ========
    {enhanced_section_main}

    ======== SIMILAR EXAMPLE ========
    This example is labeled as {trans_label}:
    {inputex}

    ======== DECISION ========
    Vulnerable
    or
    Non-vulnerable
    """


    start_time = time.time()
    response = call_ollama_api(
        model="modelscope.cn/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
        prompt=prompt_content,
        num_ctx=15000
    )
    end_time = time.time()

    prediction = response.get('message', {}).get('content', '') or response.get('response', '')


    input_tokens = response.get('prompt_eval_count', 0)
    output_tokens = response.get('eval_count', 0)
    elapsed = response.get('total_duration', 0) / 1e9

    return {
        "prediction": prediction.replace("\n", "").strip(),
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "elapsed": elapsed
    }



def calculate_metrics(predictions, ground_truth):
    tp = sum((p == 1 and t == 1) for p, t in zip(predictions, ground_truth))
    tn = sum((p == 0 and t == 0) for p, t in zip(predictions, ground_truth))
    fp = sum((p == 1 and t == 0) for p, t in zip(predictions, ground_truth))
    fn = sum((p == 0 and t == 1) for p, t in zip(predictions, ground_truth))

    accuracy = (tp + tn) / len(predictions)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    valid_idx = [i for i, p in enumerate(predictions) if p in (0, 1)]
    if valid_idx:
        preds_valid = [predictions[i] for i in valid_idx]
        gts_valid = [ground_truth[i] for i in valid_idx]
        mcc = matthews_corrcoef(gts_valid, preds_valid)
        try:
            auc = roc_auc_score(gts_valid, preds_valid)
        except ValueError:
            auc = 0.0
    else:
        mcc, auc = 0.0, 0.0

    return accuracy, precision, recall, f1, mcc, auc



def run_for_alpha(alpha, restructured_path, summary_writer=None, max_sim_slices=20):
    base_dir = '/ai/dataset/qsedata/GRACE/GRACE-main/gen_example/Reveal/sim_code_addstructure_stage1_topk=5_512'
    source_data_path = os.path.join(base_dir, f'Reveal_slice_test_stageA_{alpha}.json')

    log_path = f'log/Reveal/RevealmetricsQwen3-coder-30B_stage1_{alpha}_topk=5_512_withslice.log'
    result_middle = f'result/Reveal/RevealresultsQwen3-coder-30B_stage1_{alpha}_topk=5_512_withslice.csv'
    result_final = f'result/Reveal/RevealresultsQwen3-coder-30B_stage1_{alpha}_topk=5_512_withslice_final.csv'

    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    os.makedirs(os.path.dirname(result_middle), exist_ok=True)

    logger = logging.getLogger(f"alpha_{alpha}")
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(log_path)
    fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(fh)

    with open(source_data_path, 'r') as f:
        data = json.load(f)

    restructured_data = load_restructured_results(restructured_path)

    predictions, ground_truths = [], []
    total_input_tokens, total_output_tokens, total_time = 0, 0, 0

    with ThreadPoolExecutor(max_workers=1) as executor:
        futures = {executor.submit(process_sample, row, restructured_data, max_sim_slices): fn for fn, row in data.items()}

        with open(result_middle, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Filename', 'Prediction', 'Ground Truth'])

        for future in tqdm(as_completed(futures), total=len(futures), desc=f"处理样本 α={alpha}"):
            fn = futures[future]
            try:
                result = future.result()
                prediction = result["prediction"]
                total_input_tokens += result["input_tokens"]
                total_output_tokens += result["output_tokens"]
                total_time += result["elapsed"]

                target = int(data[fn]['label'])

                with open(result_middle, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([fn, prediction, target])

                if prediction == "Non-vulnerable":
                    prediction_val = 0
                elif prediction == "Vulnerable":
                    prediction_val = 1
                else:
                    prediction_val = 2

                predictions.append(prediction_val)
                ground_truths.append(target)
            except Exception as e:
                logger.error(f"样本处理失败: {fn} - {str(e)}")


    with open(result_final, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Prediction', 'Groundtruth'])
        writer.writerows(zip(predictions, ground_truths))

    acc, pre, rec, f1, mcc, auc = calculate_metrics(predictions, ground_truths)

    logger.info(f"Accuracy: {acc:.6f}")
    logger.info(f"Precision: {pre:.6f}")
    logger.info(f"Recall: {rec:.6f}")
    logger.info(f"F1: {f1:.6f}")
    logger.info(f"MCC: {mcc:.6f}")
    logger.info(f"AUC: {auc:.6f}")
    logger.info(f"Input tokens: {total_input_tokens}")
    logger.info(f"Output tokens: {total_output_tokens}")
    logger.info(f"Total time: {total_time:.2f}s, Avg per sample: {total_time/len(data):.2f}s")

    print(f"α={alpha} -> Acc={acc:.4f}, Prec={pre:.4f}, Rec={rec:.4f}, F1={f1:.4f}, MCC={mcc:.4f}, AUC={auc:.4f}, "
          f"Tokens(in/out)={total_input_tokens}/{total_output_tokens}, Time={total_time:.1f}s")

    if summary_writer:
        summary_writer.writerow([
            alpha, acc, pre, rec, f1, mcc, auc,
            total_input_tokens, total_output_tokens,
            total_time, total_time / len(data)
        ])



def main():
    restructured_path = "/ai/dataset/qsedata/GRACE/GRACE-main/gen_example/Reveal/restructured_results.json"
    alpha_values = [round(x * 0.1, 1) for x in range(11)]
    summary_file = "result_summary/Reveal/metrics_summary_withslice.csv"
    os.makedirs(os.path.dirname(summary_file), exist_ok=True)

    with open(summary_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([
            "Alpha", "Accuracy", "Precision", "Recall", "F1", "MCC", "AUC",
            "Total_Input_Tokens", "Total_Output_Tokens",
            "Total_Time(s)", "Avg_Time_per_Sample(s)"
        ])
        for alpha in alpha_values:
            run_for_alpha(alpha, restructured_path, summary_writer=writer)
            f.flush()  


if __name__ == "__main__":
    main()
